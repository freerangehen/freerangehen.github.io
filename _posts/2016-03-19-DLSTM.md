---
layout: post
title: Music generation with Recurrent Neural Networks
---

#blah replace `` with ** for emphasis 


["Deep learning"](https://en.wikipedia.org/wiki/Deep_learning) describes the use multilayer computational architectures, typically of Neural Networks, to capture a `hierarchy` of features embedded within the data. Some of these techniques have achieved super human performance in recent years on pattern recognition tasks. This post documents my experiments on implementing a multilayer recurrent neural network for learning musical patterns and generation. 

### Machine learning with Neural Networks

A typical Neural Network architecture consists of Neurons computing the weighted sum of it's inputs and subsequently "squashing" the weighted sum with a non-linear function such as the hyperbolic tangent:

$$ y_{[out_i]} = tanh\left[ \sum_{\text{neuron inputs }u^{(k)} } \left(w_{ik} u^{(k)}\right) + bias_i  \right]$$ 

The squashing non-linearity is what makes the generalisation property of neural networks possible. For the hyperbolic tangent function, we can say that it groups its input into three categories, for large inputs, no matter how large, the tanh always evaluates to 1. Whereas for negative quantities, no matter how negative, the tanh function evaluates to -1. Outputs computed from input values near zero scales approximately linearly with the input. This means a large number of input values are mapped to the same output value. And if the neuron is trained to "group" all meaningful inputs to the same output, it possesses generalisation properties. The weights of each neuron adjusts the "preferred pattern" of the neuron. With the notation $$y_{[out_i]}$$ denoting the $$i^{th}$$ output neuron with it's own weights $$w_{i1}, w_{i2},...$$ - the entire layer of neurons can be specified by the parameters $$w_{ik}$$ dictating how we scale the $$k^{th}$$ input for contributing to the $$i^{th}$$ output. Suppose a neuron is trained to "detect" (output firing) a large input at $$u^{(1)}$$ and a smaller input at $$u^{(2)}$$, the corresponding weight $$w_{i2}$$ will be large to amplify the effect of $$u^{(2)}$$ whereas the weight $$w_{i1}$$ will be small to ensure small $$u^{(1)}$$ inputs does not contribute to the neuron firing (i.e. output = 1). The "preferred input pattern" for a neuron to fire is termed [receptive field](https://en.wikipedia.org/wiki/Receptive_field) in neural science. In computing, the receptive field of an artificial neuron can be inferrred by visualising the weights of the trained neuron, often giving us a clue on what pattern this neuron is trained to detect. In the field of machine pattern recognition, raw data is often processed to extract useful "features" before a classifier. In most cases, the selection of features determines the performance of the system. The beauty of deep network architectures is the multilayer cascaded network is trained to extract useful, hierahical features of the input data that improves classification accuracy of later layers. This is learnt with the training data through an optimisation process. In a lot of cases this can be done without an extensive understanding of the data and without having to decide on the best set of features to be used. Ofcourse, a thorough understanding of the data and task at hand would help to train a better network by choosing a architecture that fits the data without too much redundancies or having to resort to the expensive fully connected nets. (e.g. localised receptive fields in [ConvNets](https://en.wikipedia.org/wiki/Convolutional_neural_network) for image recognition). 

Multilayer Neural Networks are constructed by cascading layers of networks with the output of one layer feeding into the input of the next layer. It is often useful to see the output of a NN as a probability. For this, the softmax function is soften applied across the output layer:

>$$ P(\text{neuron}_i \text{ firing}) = \frac{e^{y_i}}{\sum_{i}e^{y_i}} $$

thus we take the exponential of each of the neuron outputs and compute the sum of all these exponentials and use that as a normalisation constant such that all probabilities adds up to unity. 

With the Neural Network architecture fixed, one would like to assign appropriate weights/parameters to the network for the task at hand. Often we would like the network to produce the desired output given some inputs examples (training set). To measure how well a network performs, we can use the `squared error` cost function:

>$$
C = \frac{1}{2}\sum_{j\in \text{output index}} \left( y_j - k_j \right)^2
$$

where $$k_j$$ is the example output of the $$j^{th}$$ neuron. Sometimes we would like to maximise the `probability` of obtaining the desired output pattern. This can be formulated as an alternative __cross entropy__ cost function between the desired output pattern (again normalised as a probability vector $$\underline{P} = {P_1, P_2, P_3,...}$$) and the network output softmax probability:

>$$ L_{\text{X-Entropy}} = -\sum_{j\in \text{output index}} P_jln\left[\frac{e^{y_j}}{\sum_i e^{y_i}}\right] $$

To minimise the cross-entropy loss we differentiate the loss function w.r.t the weights of the network and update weights at each training iteration according to:

>$$ w_{ik} \vert_{new} = w_{ik} \vert_{old} + \text{ update rate } \frac{\partial L}{\partial w_{ik}} $$

Over a number of weight updates and with an appropriately small update rate and meaningful inputs (not just random noise), the network would normally converge to a local minimum of the cost function. This is known as __gradient descent__, a common optimisation process for tuning model parameters to minimise a "cost". The gradiet $$ \partial L / \partial w_{ij}$$ can be derived using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):

>$$ \frac{\partial L}{\partial w_{ik}} = \sum_{y_i} \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \omega_{ik}} $$

This way of evaluating the partial derivative of the cost function $$L$$ w.r.t the weights is known as *back propagation*. For a simple one layer feedforward network that simply computes the squashed weighted input sum, we can derive each of the partial derivative terms for weight updates for the cross entrypy cost function:

$$ \frac{\partial L}{\partial y_i} = \frac{\partial}{\partial y_i} \left[-\sum_{j\in \text{output index}} P_jln\left[\frac{e^{y_j}}{\sum_i e^{y_i}}\right] \right] $$

We can derive an expression in terms of $$y_i$$ and $$P_j$$:

$$
\frac{\partial L}{\partial y_i} = 
\frac{\partial}{\partial y_i} \left[-\sum_{j\in \text{output index}} \left[ P_j y_j - P_j ln\left( \sum_i e^{y_i} \right) \right] \right]
$$

$$
=>\frac{\partial L}{\partial y_i} = -P_i + \sum_{j} P_j \frac{\partial}{\partial y_i}\left[ ln \left( \sum_i e^{y_i} \right) \right]
$$

$$
=>\frac{\partial L}{\partial y_i} = -P_i + P_1 \frac{\partial}{\partial y_i}\left( ln \sum_i e^{y_i} \right)
+ P_2 \frac{\partial}{\partial y_i}\left( ln \sum_i e^{y_i} \right) + ... + + P_J \frac{\partial}{\partial y_i}\left( ln \sum_i e^{y_i} \right)
$$

$$
=>\frac{\partial L}{\partial y_i} = -P_i + P_1 \frac{e^{y_i}}{\sum_i e^{y_i}} + P_2 \frac{e^{y_i}}{\sum_i e^{y_i}} +...
+ P_J \frac{e^{y_i}}{\sum_i e^{y_i}} 
$$

$$
=>\frac{\partial L}{\partial y_i} = 
= -P_i +\underbrace{\left( P_1 + P_2 +...+P_J \right)}_{=1} \overbrace{\frac{e^{y_i}}{\sum_i e^{y_i}}}^{\text{network out softmax}}
$$

This evaluates to:

>$$
\frac{\partial L}{\partial y_i} = \text{softmax}\left( y_i \right) - P_i
$$

thus the partial derivative ("error") at each of the $$0,1,..i,...$$ output neurons is computed by the difference in network computed probability $$\text{softmax}(y_i)$$ and the example probability $$P_i$$ given the cross-entrypy as a cost function. The cost derivative $$ \partial L / \partial \omega_{ij} $$w.r.t. the weight $$\omega_{ij}$$ can be computed via the chain rule with $$ \partial y_i / \partial u^{(k)} $$. Using the simple single layer network as an example, we obtain:

>$$
\frac{\partial y_i}{\partial u^{(k)}} = tanh'\left[ \sum_{\text{neuron inputs }u^{(k)} } \left(w_{ik} u^{(k)}\right) + bias_i  \right]u^{(k)}
$$

This partial derivative of the loss function can also be the summed over a number of training examples. i.e. evaluating 

$$\sum_{\text{training e.g.}} \frac{\partial L\vert _{e.g.}}{\partial w_{ik}}$$

This process is known as `mini batch training` and can speed up convergence significantly. 

So far we have assumed a cross-entropy cost function. For the squared error cost, we have:

$$
\frac{\partial C}{\partial y_i} = y_i - k_i = i^{th}\text{ neuron output } - \text{ example output of } i^{th} \text{ neuron} 
$$

For more complicated networks, the "error" signal can be propagated back using the same partial differentiation process. We will demonstrate this again when we derive the update derivatives for a recurring network.


### Recurrent Networks

The feedforward network described above takes the input layer and performs nonlinear transformation to produce the output. Recurrent Networks are Neural Networks that computes its outputs not just with the network input but also with the previous network states or outputs. This memorising capability allows recurrent networks to handle many interesting tasks including music generation as we've experimented with here. Simple recurrent networks computes outputs of the $$i^{th}$$ neuron according to:

$$
y(t)_i = tanh\left[ \sum_{states x^{(k)}_t} \omega^{out}_{ik}x^{(k)}_t \right] \text{, with}
$$
  
$$  x^{(i)}_{t} = tanh \left[ \sum_{k^{th}\text{ inputs }u_k } \left(w^{in}_{ik} u_t^{(k)} \right) + \sum_{\text{state at t-1: } x_{t-1}^{(k)} } \left(w^{x}_{ik} x_{t-1}^{(k)} \right) + bias_k  \right]  $$

The index $$i$$ here is used to denote the $$i^{th}$$ state $$x_t^{(i)}$$ in the same layer (assuming a single layer for now). Note for each time step we use the same weights and bias scaling. 

So we've seen how error can be back propagated towards the input layers and the derivative of the cost function with respect to the parameters evaluated on a feedforward network. For recurrent networks, this is slightly more complicated. At each time step $$t=0,1,2...$$, an output $$y(0), y(1), y(2),...$$ is computed by the network whereas for the purpose of training, an example output $$k(0), k(1), k(2),...$$ is provided. Thus at each time step there is an error/cost function we can compute accoring to $$E_t(y(t),k(t))$$ that we can use to calculate the parameter updates by computing $$\partial E_t/partial \theta$$. To generate an error signal $$E_t$$ at time $$t$$, the recurrent network computes the states from time $$0,1,...,t$$ as each state $$x(0), x(1), ...$$ is feedfoward in time for the next computation, each time advancement stage thus involves the same parameters $$\theta$$. On the other hand, each parameter $$\theta$$ contributes to the error $$E_t$$ at time $$t$$ through $$0,1,...,t$$ steps. The derivative of the parameter w.r.t. error $$E_t$$ at time $$t$$ can be derived according to the chain rule:

$$ \frac{\partial E_t}{\partial \theta} = \frac{\partial E_t}{\partial x_{t-1}} \frac{\partial x_{t-1}}{\partial \theta} + \frac{\partial E_t}{\partial x_{t-2}} \frac{\partial x_{t-2}}{\partial \theta} + ... + \frac{\partial E_t}{\partial x_{0}} \frac{\partial x_{0}}{\partial \theta}
=\sum_{1\leq \tau \leq t-1}\frac{\partial E_t}{\partial x_{\tau}} \frac{\partial x_{\tau}}{\partial \theta}
$$

We can expand the expression $$\partial E_t / \partial x_{\tau}$$ by using the chain rule again:

$$
\frac{\partial E_t}{\partial \theta} =  
=\sum_{1\leq \tau \leq t-1}\underbrace{\left[\frac{\partial E_t}{\partial x_{t-1}} \frac{\partial x_{t-1}}{\partial x_{t-2}} ... \frac{\partial x_{\tau +2}}{\partial x_{\tau +1}}
\frac{\partial x_{\tau +1}}{\partial x_{\tau }}\right]}_{\partial E_t / \partial x_{\tau}}
\frac{\partial x_{\tau }}{\partial \theta}
$$

This is the partial derivative of the cost function evaluated at time $$t$$ w.r.t parameter $$\theta$$. The total derivative of the total cost $$E$$ w.r.t the parameter is given by the sum across all cost at times $$t=0,1,2,..$$:

$$ \frac{dE}{d\theta} = \sum_{t=1..T} \frac{\partial E_t}{\partial \theta}$$

The gradient update for each parameter are carried out iteratively according to:

$$ \theta \vert _{new} = \theta \vert _{old} - \left[\text{update rate}\right] \frac{dE}{d\theta}$$

The error signal at each time step contributes to the partial derivative of each parameter 

Thus practically we compute the "error" at the output and derive back propagation expressions for each weight parameter. We will futher illustrate this process in our three layer recurrent network.

The chain rule facilitates computing the partial derivatives for weight updates through a product function of the partial derivative terms. For the simple single layer example, everytime we back propagate in time using the partial derivative $$\partial x_{\tau+1} / \partial x_{\tau}$$ we multiply the extra term towards the final gradient:

$$
\frac{\partial x^{(i)}_{\tau+1}}{\partial x^{(k)}_{\tau}} = 
tanh' \left[ \sum_{k^{th}\text{ inputs }u_k } \left(w^{in}_{ik} u_{\tau}^{(k)} \right) + \sum_{\text{state at } \tau \text{-1: } x_{\tau - 1}^{(k)} } \left(w^{x}_{ik} x_{\tau - 1}^{(k)} \right) + bias_k  \right]\omega^{(x)}_{ik}
$$

The derivative of the tanh function evaluates to $$0 \leq tanh'(\bullet ) \leq 1$$ regardless of it's argument. For back propagation through time we can have the product of many terms. As a result, the product is likely to evaluate to a very small number, rendering the gradient contribution of errors at later time steps approaching zero. This means that the recurrent network will not have the correct updates for learning data with important temporal events separated by a long duration in time. This is known as the *vanishing gradient* problem in training recurrent networks as well as deep feedforward networks, where the error have to back propagate across a large number of layers following the chain rule and results in a large number of product terms. 

The vanishing gradient is one of the main hurdles of training recurrent networks with gradient based methods as the gradient updates do not fully reflect the need to steer the cost function in the correct direction towards a local minima. This is deeply rooted into the network architecture of using activation function such as the hyperbolic tangent. A number of remidies have been proposed to tackle the vanishing gradient updates, one of them being the Long-Short term memory architecture, where a linear memory unit is used without activation. The error propagation from the memory unit itself at a later time thus do not suffer from a vanishing gradient. The short commings of using linear units in a neural network is compensated by memory "gate" units controlling whether the linear (memory) unit is updated according to the input and whether the linear unit is propagated to the output. So the "Neuron" unit in a LSTM is different from those of the traditional RNNs. The general LSTM architecuture specifies gate sharing with the same gate controlling a number of linear units. A graphical description of a single "neuron" cell of the LSTM is shown below. The memory variable $$c_t$$ is feedback to itself without nonlinearity, hence the gradient do not "vanish" over time. The gate and output unit computation involves either a logistic function or tanh nonlinearity and in principle will still suffer from vanishing gradient to some extent but have shown extreme effectiveness in practice for picking up temporal events separated by a long time lag.

![LSTMcell](/images/LSTMcell.png){:.some-css-class width="600"}


We have dropped the layer indication such that the cell described above can belong to any layer of a deep network. $$h_t$$ denotes the output of a hidden unit in the deep architecture. The overall output of the deep network is computed as the weighted sum of a number of these hidden units.

### Long-Short Term Memory RNNs

In a multilayer (*deep*) LSTM, connecting the output of each layer to the overall network output directly aids gradient back propagation. The overall network architecture is illustrated below:

![LSTM3layer](/images/3layer.png){:.some-css-class width="500"}

Using column vectors for the layer variables and weight matrixes while denoting element-wise multiplcation by $$\odot$$, the forward pass of the three layer LSTM is computed by:

$$\underline{i}_t^j = g_i\left[ \underline{I}_t^j \right]
=g_i\left[ \boldsymbol{W}_{xi}^j \underline{x}_t +
\boldsymbol{W}_{xj}^j \underline{h}_t^{j-1} + \boldsymbol{W}_{hi}^j \underline{h}_{t-1}^j +
\boldsymbol{W}_{ci}^j \underline{c}_{t-1}^j + \underline{b}_i^j
\right]$$ for $$j=1,2,3$$

$$\underline{f}_t^j = g_f\left[ \underline{F}_t^j \right]
=g_f\left[ \boldsymbol{W}_{xf}^j \underline{x}_t +
\boldsymbol{W}_{fj}^j \underline{h}_t^{j-1} + \boldsymbol{W}_{hf}^j \underline{h}_{t-1}^j +
\boldsymbol{W}_{cf}^j \underline{c}_{t-1}^j + \underline{b}_f^j
\right]$$ for $$j=1,2,3$$

$$ \underline{c}_t^j = \underline{f}_t^j \odot \underline{c}_{t-1}^j + 
\underline{i}_t^j \odot tanh\left[ \underline{C}_t^j \right]
\text{, }\underline{C}_t^j = \boldsymbol{W}_{xc}^j \underline{x}_t +
\boldsymbol{W}_{cj}^j \underline{h}_t^{j-1} + \boldsymbol{W}_{hc}^j \underline{h}_{t-1}^j 
 + \underline{b}_c^j $$ for $$j=1,2,3$$ 

$$\underline{o}_t^j = g_o\left[ \underline{O}_t^j \right]
=g_o\left[ \boldsymbol{W}_{xo}^j \underline{x}_t +
\boldsymbol{W}_{oj}^j \underline{h}_t^{j-1} + \boldsymbol{W}_{ho}^j \underline{h}_{t-1}^j +
\boldsymbol{W}_{co}^j \underline{c}_{t}^j + \underline{b}_o^j
\right]$$ for $$j=1,2,3$$

$$ \underline{h}_t^j = \underline{o}_t^j \odot tanh\left[ \underline{c}_t^j \right] $$ for $$ j = 1,2,3$$

The network output is computed by weighted sum of the output of all three layers:

$$\underline{y}_t = g_y\left[ \underline{Y}_t \right] =
g_y\left[ \boldsymbol{W}_{hy}^1 \underline{h}_t^1 +
\boldsymbol{W}_{hy}^2 \underline{h}_t^2 + \boldsymbol{W}_{hy}^3 \underline{h}_t^3 + \underline{b}_y
\right]$$

The three layers in our LSTM implementation are cascaded with output of one layer connected as input of the next layer. However, since the overall network output receives contribution from all three layers directly, the three layers can be thought of a cascade and parallel hybrid.



### Back propagation in time and space in LSTM


Following the chain rule, the backwards error equations are derived for each $$j=1,2,3$$ layers:

$$\delta \underline{h}_t^j = \left[ g_y'\left(\underline{Y}_t\right) \odot \delta \underline{y}_t \right]\boldsymbol{W}_{hy}^j +
\left[ g_i'\left(\underline{I}_{t+1}^j\right) \odot \delta \underline{i}_{t+1}^j \right]\boldsymbol{W}_{hi}^j +
\left[ g_f'\left(\underline{F}_{t+1}^j\right) \odot \delta \underline{f}_{t+1}^j \right]\boldsymbol{W}_{hf}^j +
\left[ g_o'\left(\underline{O}_{t+1}^j\right) \odot \delta \underline{o}_{t+1}^j \right]\boldsymbol{W}_{ho}^j +
\left[ g_i'\left(\underline{I}_{t}^{j+1}\right) \odot \delta \underline{i}_{t}^{j+1} \right]\boldsymbol{W}_{xj}^{j+1} +
\left[ g_f'\left(\underline{F}_{t}^{j+1}\right) \odot \delta \underline{f}_{t}^{j+1} \right]\boldsymbol{W}_{fj}^{j+1} +
\left[ g_o'\left(\underline{O}_{t}^{j+1}\right) \odot \delta \underline{o}_{t}^{j+1} \right]\boldsymbol{W}_{oj}^{j+1} +
\left[ \delta \underline{c}_{t+1}^j \odot \underline{i}_{t+1}^j \odot tanh'\left( \underline{C}_{t+1}^j \right) \right]\boldsymbol{W}_{hc}^j +
\left[ \delta \underline{c}_{t}^{j+1} \odot \underline{i}_{t}^{j+1} \odot tanh'\left( \underline{C}_{t}^{j+1} \right) \right]\boldsymbol{W}_{cj}^{j+1}$$ $$$$

$$\delta \underline{C}_t^j = 
\left[ \delta \underline{c}_{t}^{j} \odot \underline{i}_{t}^{j} \odot tanh'\left( \underline{C}_{t}^{j} \right) \right]
$$, with 

$$\delta \underline{c}_t^j = 
\left[ g_i'\left(\underline{I}_{t+1}^j\right) \odot \delta \underline{i}_{t+1}^j \right]\boldsymbol{W}_{ci}^j +
\left[ g_f'\left(\underline{F}_{t+1}^j\right) \odot \delta \underline{f}_{t+1}^j \right]\boldsymbol{W}_{cf}^j + \\
\left[ g_o'\left(\underline{O}_{t+1}^j\right) \odot \delta \underline{o}_{t+1}^j \right]\boldsymbol{W}_{co}^j + 
\underbrace{\delta \underline{c}_{t+1}^j}_{\text{linear bkprop}} \odot \underline{f}_{t+1}^j +
\delta \underline{h}_t^j \odot \underline{o}_t^j \odot tanh'\left( \underline{c}_t^j \right)
$$

$$\delta \underline{O}_t^j =  \delta \underline{o}_t^j \odot g_o'\left( \underline{O}_t^j \right) $$, with $$\delta \underline{o}_t^j = \delta \underline{h}_t^j \odot tanh\left( \underline{c}_t^j \right)$$


$$\delta \underline{F}_t^j = \delta \underline{f}_t^j \odot g_f'\left( \underline{F}_t^j \right) $$, with $$\delta \underline{f}_t^j = \delta \underline{c}_t^j \odot  \underline{c}_{t-1}^j$$

$$\delta \underline{I}_t^j = \delta \underline{i}_t^j \odot g_i'\left( \underline{I}_t^j \right) $$, with $$\delta \underline{i}_t^j = \delta \underline{c}_t^j \odot tanh\left( \underline{C}_t^j \right)$$

The weight updates are computed as outer products with all vectors being column vectors with row vector transposes:

$$
\Delta \boldsymbol{W}_{xi}^j = \delta \underline{I}_t^j \cdot \underline{x}_t^{T} \text{;  }
\Delta \boldsymbol{W}_{xj}^j = \delta \underline{I}_t^j \cdot \left( \underline{h}_t^{j-1} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{ci}^j = \delta \underline{I}_t^j \cdot \left( \underline{c}_{t-1}^{j} \right)^{T} 
$$

$$
\Delta \boldsymbol{W}_{xf}^j = \delta \underline{F}_t^j \cdot \underline{x}_t^{T} \text{;  }
\Delta \boldsymbol{W}_{fj}^j = \delta \underline{F}_t^j \cdot \left( \underline{h}_t^{j-1} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{hf}^j = \delta \underline{F}_t^j \cdot \left( \underline{h}_{t-1}^{j} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{cf}^j = \delta \underline{F}_t^j \cdot \left( \underline{c}_{t-1}^{j} \right)^{T} 
$$

$$
\Delta \boldsymbol{W}_{xc}^j = \delta \underline{C}_t^j \cdot \underline{x}_t^{T} \text{;  }
\Delta \boldsymbol{W}_{cj}^j = \delta \underline{C}_t^j \cdot \left( \underline{h}_t^{j-1} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{hc}^j = \delta \underline{C}_t^j \cdot \left( \underline{h}_{t-1}^{j} \right)^{T} 
$$

$$
\Delta \boldsymbol{W}_{xo}^j = \delta \underline{F}_t^j \cdot \underline{x}_t^{T} \text{;  }
\Delta \boldsymbol{W}_{oj}^j = \delta \underline{F}_t^j \cdot \left( \underline{h}_t^{j-1} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{ho}^j = \delta \underline{F}_t^j \cdot \left( \underline{h}_{t-1}^{j} \right)^{T} \text{;  }
\Delta \boldsymbol{W}_{co}^j = \delta \underline{F}_t^j \cdot \left( \underline{c}_{t}^{j} \right)^{T} 
$$

$$
\Delta \boldsymbol{W}_{hy}^j = \delta \underline{Y}_t \cdot \left( \underline{h}_t^{j} \right)^{T}
$$


### Music generation with LSTM and Practical issues

One cool application of deep recurrent neural networks is music generation. Since I hardly know how to play any instruments, I figure it would be cool to train a NN to generate some music.

I implemented the three layer LSTM using the Python/[Theano](http://www.deeplearning.net/software/theano/) (0.7) framework. Gradients were calculated using the equations above without relying on the symbolic differentiation of Theano. The main reason is that symbolic differentation took too long to compile with my setup. The manually derived error terms also relied on assumptions such as $$\underline{o}_t^j$$ is independent of $$\underline{c}_t^j$$ while calculating $$\partial \underline{h}_t^j / \partial \underline{c}_t^j$$. Implementing these assumptions in Theano's symbolic differentiation requires experimenting with "constant settings" in the gradient routine. Implementing the equations directly using Theano tensor operations were also fast enough. A rolling average of the gradient were used for each update (RMSprop). The three layer network in the experiment have 120 hidden units for each layer. A squared error cost function is used in the training. The network is initialised with all weights set to zero and a small gaussian noise to break the symmetry. All bias were set such that the network starts in the middle of all nonlinearities. E.g. for the logistic function, biases are initialised to $$0.5\div [\text{#neurons in layer}]$$ with a small gaussian noise whereas for tanh activated neurons, biases are initialised around zero with small amount of noise added. The network is trained with [Nottingham database of folk tunes](http://www.iro.umontreal.ca/~lisa/deep/data/Nottingham.zip) using the [link](http://deeplearning.net/tutorial/rnnrbm.html) from the official Theano example exploring a different RNN-RBM architecture. The network was trained using each midi file as a minibatch. Each minibatch was trained for only a few epochs (10) as subsequent epochs of the same minibatch does not produce significant reduction in the cost function. The network has been trained through the entire dataset of 694 examples once before it was used for music generation. Further training without further regularisation over trains the network.

To generate music using a trained network, we run through one of the training pieces as input input and subsequently connect the network output as input at the next time step. The output at next time step is generated with the current output feed as network input for the next time step. A typical generated piece is shown below:

![gened631](/images/gened631.png){:.some-css-class width="2000"}

The original training data for generating the above piece consists of a large number of consecutive notes at midi note #38 and #44. Hence the probability of generating the two notes are generally high. The typical probability of note production of the network is shown below:

![prob631](/images/prob631.png){:.some-css-class width="1000"}

There is a clear pattern in the probability evolution with only a subset of the possible notes having a higher probability of being produced by the network. We find that with more training, the network exhibits over-training and the generation probability of each note becomes constant and resembles the average appearance probability of each note in the training set. Perhaps randomising weights and using the cross entropy cross function may prevent over training and allow the network to pick up more subtle patterns. Also training the network with a more musically diverse dataset might produce more pleasant sounding results.




### References

Papers, thesis, and videos that helped me along (almost essential!) with the experiment: 

1. This work is largely inspired by the official [Theano example](http://deeplearning.net/tutorial/rnnrbm.html) on music generation, which uses RBM and a classical RNN architecture. Also useful are the recommended Python Midi library in the link.

2. Great [lecture](https://class.coursera.org/ml-003/lecture) by Andrew Ng on back propagation. 


2. The Deep LSTM architecture with direct output/input connections to each layer is taken from Alex Graves ["Generating Sequenes with Recurrent Neural Networks"](http://arxiv.org/abs/1308.0850) Neural and Evolutionary Computing

3. Chapter 2 of Alex Graves' thesis ["Supervised Sequence Labelling with Recurrent Neural Networks"](http://deeplearning.cs.cmu.edu/pdfs/1104/Supervised_Sequence_Labeling.pdf) presented the derivation of the back prop equations for more general cases.

4. Jeffrey Hinton's [video lecture](https://www.youtube.com/watch?v=defQQqkXEfE&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=29) on mini batch training, Rprop, and RMSprop. 







